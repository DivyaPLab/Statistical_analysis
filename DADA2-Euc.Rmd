---
title: "DADA2-Euc"
author: "Divya"
date: "2022-11-03"
output: html_document
---
## We'll start off by removing all the data we might have loaded before

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, include = FALSE)

rm(list=ls())
```

## Opening DADA2 and setting our working directory for our Eukaryote (Euc) files

```{r message=FALSE, warning=FALSE, include=FALSE}
library(dada2); packageVersion("dada2")

#Setting a path to the FASTQ files
#Here I'm focusing on my Eukaryotes
path <-"~/sequences/Euc/euc_fastq"


#Just going to make sure the path is leading to the .fastq files
list.files(path)

#Alrigth, looks like it worked! ^.^
```

## Seperate the Forward and Reverse fastq files

```{r pressure, include=FALSE}
# Identify forward and reverse reads
# Filenames containing R1 are forward reads, R2 are reverse reads.
fnFs <- sort(list.files(path, pattern="R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="R2_001.fastq", full.names = TRUE))
```

## Assign the filenames for the filtered fastq.gz files and place in filtered subdirectory 
```{r}
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

# We'll still have the "-bac" at the end of the file names. 
#To remove that, we'll use gsub()
sample.names <- gsub("-euc", "", sample.names)

# Let's make sure the previous functions worked
sample.names

#Ok we did it. Now, let's put the files in a subdirectory
filt_path <- file.path(getwd(), "~/sequences/Euc")
filtFs <- file.path("~/sequences/Euc/filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path("~/sequences/Euc/filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
``` 


## Let's check the quality of our sequences

```{r}
#First, we can look at some of the Forwards sequences for Filter samples
plotQualityProfile(fnFs[9:18])

#Save as a PDF file
ggsave("sequences/Euc/euc_Figures/Euc_qualityplot_Filtre_Forward.pdf", height = 8, width = 12, dpi = 16, plot = last_plot())

#Then, some of the Forwards sequences for Rock samples 
plotQualityProfile(fnFs[40:50])
ggsave("sequences/Euc/euc_Figures/Euc_qualityplot_Roche_Forward.pdf", height = 8, width = 12, dpi = 16, plot = last_plot())


#Now we can look at some of the Reverse sequences for Filter samples
plotQualityProfile(fnRs[9:18])
ggsave("sequences/Euc/euc_Figures/Euc_qualityplot_Filtre_Reverse.pdf", height = 8, width = 12, dpi = 16, plot = last_plot())


#And finally, some of the Reverse sequences for Rock samples
plotQualityProfile(fnRs[40:50])
ggsave("sequences/Euc/euc_Figures/Euc_qualityplot_Roche_Reverse.pdf", height = 8, width = 12, dpi = 16, plot = last_plot())

```


## Time to trim the reads thanks to the quality graphs (see previous section of code) 
## From the previous section, it looks like the quality diminishes at the 270 mark.
## For the forward sequences, we'll remove the first 17 nucleotides (size of the primer)
## For the reverse, we'll remove the first 21 nucleotides (size of the primer).
## We'll trim the last nucleotides at position 270 (trimming the last 70 nucleotides) to avoid errors
```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(270,200),
                     maxN=0, maxEE=c(5,5), truncQ=2, trimLeft=c(17,21),
                     compress=TRUE, multithread=TRUE) 
out
```

## Let's now look at the error rate

```{r}
errF <- learnErrors(filtFs, multithread=TRUE, randomize=TRUE)
###101159773 total bases in 399841 reads from 36 samples will be used for learning the error rates.

errR <- learnErrors(filtRs, multithread=TRUE, randomize=TRUE)
###100791141 total bases in 563079 reads from 42 samples will be used for learning the error rates.

#Plot the error rates
plotErrors(errF, nominalQ=TRUE)
dev.print(width = 20, height = 10, pdf,"/home/dpatel/sequences/Euc/euc_Figures/plotErrors.pdf")
```

## Time to dereplicate: removing replicate of identical sequences from each sample

```{r}
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)

# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names

# Apply the core sample inference algorithm to the filtered and trimmed sequence data
dadaFs <- dada(filtFs, err=errF, pool="pseudo", multithread=TRUE) 
dadaRs <- dada(filtRs, err=errR, pool="pseudo", multithread=TRUE)

# Inspect the returned dada-class object:
dadaFs[[7]]

### Result = dada-class: object describing DADA2 denoising results
### 6 sequence variants were inferred from 419 input unique sequences.

```



## We're going to create the ASVs 
```{r}

# First, we merge the paired reads
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs)

# Before continuing, let's look at the data.frame (first sample only)
head(mergers[[1]])

# Construct an amplicon sequence variant table (ASV) table and check the dimension of it
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
###[1]    62 868

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
###The majority of the sequences are 253 nucleotides


# Remove chimeras 
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE)

#View dimension of your matrices 
dim(seqtab.nochim)
sum(seqtab.nochim)/sum(seqtab)
###[1]   62 574
###[1] 0.895732
``` 

## Track reads through the pipeline
```{r}
# Obtain the count of how many sequences were deleted at each steps. 
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
track
``` 

## Time to asing taxonomy to our ASVs

```{r}
# Identifying the taxonomy
taxa <- assignTaxonomy(seqtab.nochim, "~/silva_132.18s.99_rep_set.dada2.fa.gz?download=1", multithread=TRUE, tryRC=TRUE)
taxa.sp <- addSpecies(taxa,  "~/silva_species_assignment_v138.1.fa.gz?download=1", allowMultiple = TRUE, tryRC = TRUE)

# Removing sequence rownames for display only
taxa.print <- taxa 
rownames(taxa.print) <- NULL
head(taxa.print, n=20)

View(taxa)
```


## Save as .CSV files and save the sequence table

```{r}

# For the taxonomy table
write.csv(as.data.frame(taxa.print), file = file.path("~/sequences/Euc/euc_csv/EUC_S1_taxonomy_table.csv")) 

# For the ASV table
write.csv(as.data.frame(seqtab.nochim),file = file.path("~/sequences/Euc/euc_csv/ASV_euc1_table.csv")) 
write.csv(as.data.frame(t(seqtab.nochim)),file = file.path("~/sequences/Euc/euc_csv/ASV_E1_t_table.csv"))

#Save sequence table and taxonomic annotations to individual files
saveRDS(seqtab.nochim, file = file.path("~/sequences/euc1_seqtab_nochim.rds"))
saveRDS(taxa, file = file.path("~/sequences/euc1_taxa_sp.rds"))
```


## Loading libraries needed for the cleaning and summarizing of our DADA2 results

```{r}
# Let's open all the data we'll need  
library(picante)
library(vegan)
library(ggplot2)
library(reshape2)
library(ggpubr)
library(DESeq2)
library(pheatmap)
library(phyloseq)
```

## Let's also get our DADA2 results

```{r}
# load sequence table (nonchimeric ASV abundances in samples)
seqtab.nochim <- readRDS("/home/dpatel/sequences/Euc/euc1_seqtab_nochim.rds")

# load taxonomic annotations (taxonomic ID of each ASV)
taxa.sp <- readRDS("/home/dpatel/sequences/Euc/euc1_taxa_sp.rds")
```

## Let's create a community and taxonomy object

```{r}
# community object from nonchimeric ASV sequence table
comm <- seqtab.nochim

# taxonomy object - make sure order of ASVs match between community and taxonomy
taxo <- taxa.sp[colnames(comm),]
```



## Doing some cleaning

```{r}
# First, let's make sure we only have Eukaryote phylums in our taxonomy
table(taxo[,"Kingdom"])

#Let's also put our taxo as a data frame to be able to use phyloseq.
#We can also use this opportunity to put "NAs".

taxid <- as.data.frame(t(taxo))
taxid[] <- lapply(taxid, as.character)
taxid2<- tidyr::fill(taxid, colnames(taxid),.direction = "down")
taxid2<- sapply(taxid2, function(x){paste0("unclassified_", x)})
taxid[is.na(taxid)] <- taxid2[is.na(taxid)]
taxid <- t(taxid)
taxid[ taxid == "unclassified_NA" ] <- NA

# Remove unclassified ASV from the taxonomy matrix then from the ASV matrix
taxid <-subset(as.data.frame(taxid), Kingdom =="Eukaryota")
comm <- seqtab.nochim[,colnames(comm) %in% rownames(taxid)]


```


## Looking at our data.

```{r}
# Number of reads per sample
rowSums(comm)

# visualize log10 number of reads per sample
hist(log10(rowSums(comm)))
#We have a lot of reads in some samples and very few reads in other (most likely the control samples)

# log10 of number of reads per ASV
hist(log10(colSums(comm)))

# Looks like we're going to have a couple of dominant ASVs in our samples
```


## Visualize community composition

```{r}
# PCA on Hellinger-transformed community data
comm.pca <- prcomp(decostand(comm,"hellinger"))

# plot ordination results
ordiplot(comm.pca, display="sites", type="text",cex=1.0)
### Our samples (except the F-R-J1) are all seperate from our Controls (CRTL) and negatives. Some of the CRTL are pretty close to the Tourb

# Surface plot - plot number of sequences/sample (library size)
ordisurf(comm.pca, rowSums(comm), bubble=TRUE, cex=2,
         main="Library size (sequences/sample)")
dev.print(width = 10, height = 8, pdf,"/home/dpatel/sequences/Euc/euc_Figures/seq_per_sample.pdf")
### Most samples are between 12 000 and 16 000 sequences. There are a couples of samples at around 0 sequence. 
```


## Check negative controls

```{r}
# Abundance of ASVs in negative controls
comm['BlancKit',][comm['BlancKit',]>0]
comm['CRTL-B4B',][comm['CRTL-B4B',]>0]
comm['CRTL-B5H',][comm['CRTL-B5H',]>0]
comm['CRTL-J1B',][comm['CRTL-J1B',]>0]
comm['CRTL-J2M',][comm['CRTL-J2M',]>0]
comm['CRTL-R3H',][comm['CRTL-R3H',]>0]
comm['CRTL-R6M',][comm['CRTL-R6M',]>0]
```


Create phyloseq containing the phylogenetic tree and the metadata 
```{r}
## PHYLOSEQ OBJECT 
library(Biostrings); packageVersion("Biostrings")
library(decontam)
library(phyloseq)

# load metadata
metadata <- read.csv("/home/dpatel/metadata_Bioreacteur1.csv", header=TRUE, sep=",", row.names = 1)

# inspect metadata
head(metadata)

#Phyloseq 
ps <- phyloseq(otu_table(t(comm), taxa_are_rows=TRUE), tax_table(as.matrix(taxid)), sample_data(metadata))

# Add sequence table 
dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)

# Make ASV ID shorter 
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
```

## Remove the negative controls (decontam)

```{r}
sample_data(ps)$is.neg <- sample_data(ps)$SampleType == "Controle"
contamdf.prev <- isContaminant(ps, method="prevalence", neg="is.neg", threshold=0.5)
table(contamdf.prev$contaminant)
ps.noncontam <- prune_taxa(!contamdf.prev$contaminant, ps)
final.ps=subset_samples(ps.noncontam, !SampleType=="Controle")

#Save the ASVs, taxa and metadata
write.csv(as.data.frame(as(tax_table(final.ps), "matrix")), file = "/home/dpatel/sequences/Euc/euc_csv/noncontam_taxa.csv") 
write.csv(as.data.frame(as(otu_table(final.ps), "matrix")),file = "/home/dpatel/sequences/Euc/euc_csv/noncontam_asv.csv")
write.csv(as.data.frame(as(sample_data(final.ps), "matrix")), file="/home/dpatel/sequences/Euc/euc_csv/noncontam_meta.csv")
```



## Visualize the new community composition

```{r}
# Load our "noncontam" files
noncontam_taxa <- read.csv("/home/dpatel/sequences/Euc/euc_csv/noncontam_taxa.csv", header = TRUE, sep =",", row.names = 1)
noncontam_asv <- read.csv("/home/dpatel/sequences/Euc/euc_csv/noncontam_asv.csv", header = TRUE, sep =",", row.names = 1)
noncontam_meta <- read.csv("/home/dpatel/sequences/Euc/euc_csv/noncontam_meta.csv", header=TRUE, sep=",", row.names = 1)

# Puttingg our ASVs and taxonomy into another object 
noncontam_comm <- t(as.data.frame(noncontam_asv))
noncontam_taxo <- as.data.frame(noncontam_taxa)[colnames(noncontam_comm),]

# PCA on Hellinger-transformed community data
noncontam.pca <- prcomp(decostand(noncontam_comm,"hellinger"))
ordiplot(noncontam.pca, display="sites", type="text",cex=1.0)
dev.print(width = 10, height = 8, pdf,"/home/dpatel/sequences/Euc/euc_Figures/PCA.pdf")
### Our samples are all pretty close from each other

```



## Making rarefaction curves 

```{r}
# What is the smallest number of sequences/sample for subset of samples?
min(rowSums(noncontam_comm))
rowSums(noncontam_comm)

# Rarefaction curve for subset of samples + saving them
rarecurve(noncontam_comm, step=200, label=TRUE)
dev.print(width = 24, height = 10, pdf,"/home/dpatel/sequences/Euc/euc_Figures/rarefaction_curve_all.pdf")

rarecurve(noncontam_comm, step=200, label=TRUE, xlim=c(0,5000))
dev.print(width = 24, height = 10, pdf,"/home/dpatel/sequences/Euc/euc_Figures/rarefaction_curve_short.pdf")

minTotRelAbun = 5e-5
x = taxa_sums(final.ps)
keepTaxa =  (x / sum(x)) > minTotRelAbun
prunedSet = prune_taxa(keepTaxa, final.ps)
sample_sums(prunedSet)
sample_sums(final.ps)

rarecurve(t(otu_table(final.ps)), step=50, cex=0.5)
ps_raref <-rarefy_even_depth(prunedSet, 3000, rngseed=112) 

#Save asv, taxa, metadata rarefied
write.csv(as.data.frame(as(tax_table(ps_raref), "matrix")), file = "/home/dpatel/sequences/Euc/euc_csv/rarefied_taxa.csv") 
write.csv(as.data.frame(as(otu_table(ps_raref), "matrix")),file = "/home/dpatel/sequences/Euc/euc_csv/rarefied_asv.csv")
write.csv(as.data.frame(as(sample_data(ps_raref), "matrix")), file="/home/dpatel/sequences/Euc/euc_csv/rarefied_meta.csv") 
```


