---
title: "DADA2-Bac"
author: "Divya"
date: "2022-11-03"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

## Installing packages

```{r setup}
knitr::opts_chunk$set(message = FALSE, include = FALSE)
# install packages from CRAN

#install.packages(pkgs = c("Rcpp", "RcppArmadillo", "picante", "ggpubr", "pheatmap"), dependencies = TRUE)
#install.packages("ANCOMBC")

# install packages from Bioconductor

# if the dada2 install returns a warning for BiocParallel, install from binary using this command:

#BiocManager::install("BiocParallel", version = "3.14", type="binary", update = FALSE)

#BiocManager::install("DESeq2")

#BiocManager::install("phyloseq")

# install dada2 if needed
#if (!requireNamespace("BiocManager", quietly = TRUE))
#  install.packages("BiocManager")
#BiocManager::install("dada2", version = "3.14")

#install package Rcpp
#install.packages("Rcpp")

#install.packages("nloptr")

```

## Opening DADA2 and setting our working directory for our Bacteria files

```{r}

library(dada2); packageVersion("dada2")


#Setting a path to the FASTQ files
#Here I'm focusing on my Bacterias

path <-"~/sequences/Bac/bac_fastq"

#Just going to make sure the path is leading to the .fastq files

list.files(path) 

#Alrigth, looks like it worked! ^.^


```


## Seperate the Forward and Reverse fastq files

```{r}

# Identify forward and reverse reads
# Filenames containing R1 are forward reads, R2 are reverse reads.

fnFs <- sort(list.files(path, pattern="R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="R2_001.fastq", full.names = TRUE))

```


## Assign the filenames for the filtered fastq.gz files and place in filtered subdirectory 

```{r}

sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

# We'll still have the "-bac" at the end of the file names. 
#To remove that, we'll use gsub()

sample.names <- gsub("-bac", "", sample.names)

# Let's make sure the previous functions worked

sample.names

#Ok we did it. Now, let's put the files in a subdirectory

filtFs <- file.path("~/sequences/Bac/filtered", paste0(sample.names, "_F_filt.fastq.gz"))

filtRs <- file.path("~/sequences/Bac/filtered", paste0(sample.names, "_R_filt.fastq.gz"))

names(filtFs) <- sample.names
names(filtRs) <- sample.names
``` 


## Let's check the quality of our sequences

```{r}
#First, we can look at some of the Forwards sequences for Filter samples

#plotQualityProfile(fnFs[9:18])

#Let's save these plots

#ggsave("sequences/Bac/bac_Figures/Bac_qualityplot_Filtres_Forward.pdf", height = 8, width = 12, dpi = 16, plot = last_plot())

#Then, some of the Forwards sequences for Rock samples 

#plotQualityProfile(fnFs[40:50])
#ggsave("sequences/Bac/bac_Figures/Bac_qualityplot_Roches_Forward.pdf", height = 8, width = 12, dpi = 16, plot = last_plot())

#Now we can look at some of the Reverse sequences for Filter samples

#plotQualityProfile(fnRs[9:18])
#ggsave("sequences/Bac/bac_Figures/Bac_qualityplot_Filtres_Reverse.pdf", height = 8, width = 12, dpi = 16, plot = last_plot())

#And finally, some of the Reverse sequences for Rock samples

#plotQualityProfile(fnRs[40:50])
#ggsave("sequences/Bac/bac_Figures/Bac_qualityplot_Roches_Reverse.pdf", height = 8, width = 12, dpi = 16, plot = last_plot())

```


## Time to trim the reads thanks to the quality graphs (see previous section of code) 
## From the previous section, it looks like the quality diminishes at the 250 mark.
## For the forward sequences, we'll remove the first 17 nucleotides (size of the primer).
## For the reverse, we'll remove the first 21 nucleotides (size of the primer).
## We'll trim the last nucleotides at position 250 (trimming the last 50 nucleotides) to avoid errors
```{r}

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(250,200),
                     maxN=0, maxEE=c(5,5), truncQ=2, trimLeft=c(17,21),
                     compress=TRUE, multithread=TRUE) 
#out

```


## Let's now look at the error rate

```{r}

errF <- learnErrors(filtFs, multithread=TRUE, randomize=TRUE)
#101502023 total bases in 435631 reads from 18 samples will be used for learning the error rates.


errR <- learnErrors(filtRs, multithread=TRUE, randomize=TRUE)
#101447176 total bases in 566744 reads from 24 samples will be used for learning the error rates.

```


## Time to dereplicate: removing replicate of identical sequences from each sample

```{r}
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)


# Name the derep-class objects by the sample names

names(derepFs) <- sample.names
names(derepRs) <- sample.names

# Apply the core sample inference algorithm to the filtered and trimmed sequence data
dadaFs <- dada(filtFs, err=errF, pool="pseudo", multithread=TRUE) 
dadaRs <- dada(filtRs, err=errR, pool="pseudo", multithread=TRUE)

# Inspect the returned dada-class object:
#dadaFs[[1]]

# Result = dada-class: object describing DADA2 denoising results
# 122 sequence variants were inferred from 7702 input unique sequences.

```



## We're going to create the ASVs 
```{r}

# First, we merge the paired reads

mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs)

# Before continuing, let's look at the data.frame (first sample only)

#head(mergers[[1]])

# Construct an amplicon sequence variant table (ASV) table and check the dimension of it

seqtab <- makeSequenceTable(mergers)
dim(seqtab)

#[1]    79 14337

# Inspect distribution of sequence lengths

#table(nchar(getSequences(seqtab)))
#The majority of the sequences are 400 nucleotides


# Remove chimeras 

seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE)


#View dimension of your matrices 

#dim(seqtab.nochim)
#sum(seqtab.nochim)/sum(seqtab)

#[1]   79 5712
#[1] 0.8088081
``` 

## Track reads through the pipeline
```{r}
# Obtain the count of how many sequences were deleted at each steps. 

getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
track
``` 

## Time to asing taxonomy. First, we need to install the database


```{r}

#Mettre dans le terminal

#wget  https://zenodo.org/record/4587955/files/silva_species_assignment_v138.1.fa.gz?download=1

#wget https://zenodo.org/record/4587955/files/silva_nr99_v138.1_train_set.fa.gz?download=1


```



## Now, we can assing the taxonomy to our ASVs

```{r}

# Identifying the taxonomy

taxa <- assignTaxonomy(seqtab.nochim, "~/silva_nr99_v138.1_train_set.fa.gz?download=1", multithread=TRUE, tryRC=TRUE)

# Adding the species for each sequence

taxa.sp <- addSpecies(taxa,  "~/silva_species_assignment_v138.1.fa.gz?download=1", allowMultiple = TRUE, tryRC = TRUE)

# Removing sequence rownames for display only

taxa.print <- taxa.sp 
rownames(taxa.print) <- NULL
head(taxa.print, n=20)

#View(taxa)
#View(taxa.sp)
#View(taxa.print)

```

## Save as .CSV files and save the sequence table

```{r}

# For the taxonomy table

write.csv(as.data.frame(taxa.print), file = file.path("~/sequences/Bac/bac_csv/BAC_S1_taxonomy_table.csv")) 


# For the ASV table

write.csv(as.data.frame(seqtab.nochim),
          file = file.path("~/sequences/Bac/bac_csv/ASV_bac1_table.csv")) 

write.csv(as.data.frame(t(seqtab.nochim)),
          file = file.path("~/sequences/Bac/bac_csv/ASV_S1_t_table.csv"))

#Save sequence table and taxonomic annotations to individual files

saveRDS(seqtab.nochim, file = file.path("~/sequences/Bac/bac1_seqtab_nochim.rds"))
saveRDS(taxa.sp, file = file.path("~/sequences/Bac/bac1_taxa_sp.rds"))

```


## Loading libraries needed for the cleaning and summarizing of our DADA2 results

```{r}
# Let's open all the data we'll need  

library(picante)
library(vegan)
library(ggplot2)
library(reshape2)
library(ggpubr)
library(DESeq2)
library(pheatmap)
library(phyloseq)

```

## Let's also get our DADA2 results

```{r}
# load sequence table (nonchimeric ASV abundances in samples)

seqtab.nochim <- readRDS("~/sequences//Bac/bac1_seqtab_nochim.rds")


# load taxonomic annotations (taxonomic ID of each ASV)

taxa.sp <- readRDS("~/sequences/Bac/bac1_taxa_sp.rds")

```

## Let's create a community and taxonomy object

```{r}
# community object from nonchimeric ASV sequence table

comm <- seqtab.nochim


# taxonomy object - make sure order of ASVs match between community and taxonomy

taxo <- taxa.sp[colnames(comm),]

```



## Doing some cleaning

```{r}
# First, let's make sure we only have bacteria phylums in our taxonomy
table(taxo[,"Kingdom"])

#Well, we have 4 Eukaryota. So those will need to be removed.
#Let's also put our taxo as a data frame to be able to use phyloseq.
#We can also use this opportunity to put "NAs".

taxid <- as.data.frame(t(taxo))
taxid[] <- lapply(taxid, as.character)
taxid2<- tidyr::fill(taxid, colnames(taxid),.direction = "down")
taxid2<- sapply(taxid2, function(x){paste0("unclassified_", x)})
taxid[is.na(taxid)] <- taxid2[is.na(taxid)]
taxid <- t(taxid)
taxid[ taxid == "unclassified_NA" ] <- NA

# Remove unclassified ASV from the taxonomy matrix then from the ASV matrix

taxid <-subset(as.data.frame(taxid), Kingdom =="Bacteria")
comm <- seqtab.nochim[,colnames(comm) %in% rownames(taxid)]

#Let's make sure we only have Bacterias

table(taxid[,"Kingdom"])

```


## Looking at our data.

```{r echo=TRUE, results='hide'}
# Number of reads per sample
rowSums(comm)

# visualize log10 number of reads per sample

hist(log10(rowSums(comm)))
#We have a lot of reads in some samples and very few reads in other (most likely the control samples)

# log10 of number of reads per ASV
hist(log10(colSums(comm)))

# Looks like we're going to have a few dominant ASV in our samples

```


## Visualize community composition

```{r echo=TRUE, results='hide'}
# PCA on Hellinger-transformed community data
comm.pca <- prcomp(decostand(comm,"hellinger"))

# plot ordination results
ordiplot(comm.pca, display="sites", type="text",cex=1.0)

# Our samples (except the Tourbière) are all seperate from our Controls (CRTL) and negatives


# Surface plot - plot number of sequences/sample (library size)

ordisurf(comm.pca, rowSums(comm), bubble=TRUE, cex=2,
         main="Library size (sequences/sample)")

# Only 6 of the samples are bellow 2 000 sequences
# Let's save the plot

pdf("~/sequences/Bac/bac_Figures/seq_per_sample.pdf")

ordisurf(comm.pca, rowSums(comm), bubble=TRUE, cex=2,
         main="Library size (sequences/sample)")
dev.off()


```


## Check negative controls

```{r echo=TRUE, results='hide'}
# Abundance of ASVs in negative controls

comm['BlancKit',][comm['BlancKit',]>0]
comm['CRTL-B4B',][comm['CRTL-B4B',]>0]
comm['CRTL-B5H',][comm['CRTL-B5H',]>0]
comm['CRTL-J1B',][comm['CRTL-J1B',]>0]
comm['CRTL-J2M',][comm['CRTL-J2M',]>0]
comm['CRTL-R3H',][comm['CRTL-R3H',]>0]
comm['CRTL-R6M',][comm['CRTL-R6M',]>0]
comm['CTRL-PCR-neg',][comm['CTRL-PCR-neg',]>0]

```


## Create phyloseq containing the phylogenetic tree and the metadata 
```{r echo=TRUE}
## PHYLOSEQ OBJECT 
library(Biostrings); packageVersion("Biostrings")
library(decontam)
library(phyloseq)


# load metadata
metadata <- read.csv("/home/dpatel/metadata_Bioreacteur1.csv", header=TRUE, sep=",", row.names = 1)

# inspect metadata
head(metadata)

#Phyloseq 

ps <- phyloseq(otu_table(t(comm), taxa_are_rows=TRUE), tax_table(as.matrix(taxid)), sample_data(metadata))


# Add sequence table 
dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)

# Make ASV ID shorter 
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))

```

## Remove the negative controls (decontam)

```{r}

sample_data(ps)$is.neg <- sample_data(ps)$SampleType == "Controle"
contamdf.prev <- isContaminant(ps, method="prevalence", neg="is.neg", threshold=0.5)
table(contamdf.prev$contaminant)
ps.noncontam <- prune_taxa(!contamdf.prev$contaminant, ps)
final.ps=subset_samples(ps.noncontam, !SampleType=="Controle")


#Save the ASVs, taxa and metadata

write.csv(as.data.frame(as(tax_table(final.ps), "matrix")), file = "~/sequences/Bac/bac_csv/noncontam_taxa.csv") 
write.csv(as.data.frame(as(otu_table(final.ps), "matrix")),file = "~/sequences/Bac/bac_csv/noncontam_asv.csv")
write.csv(as.data.frame(as(sample_data(final.ps), "matrix")), file="~/sequences/Bac/bac_csv/noncontam_meta.csv")

```



## Visualize the new community composition

```{r echo=TRUE, results='hide'}

# Load our "noncontam" files

noncontam_taxa <- read.csv("~/sequences/Bac/bac_csv/noncontam_taxa.csv", header = TRUE, sep =",", row.names = 1)
noncontam_asv <- read.csv("~/sequences/Bac/bac_csv/noncontam_asv.csv", header = TRUE, sep =",", row.names = 1)
noncontam_meta <- read.csv("~/sequences/Bac/bac_csv/noncontam_meta.csv", header=TRUE, sep=",", row.names = 1) [,-7]

# Puttingg our ASVs and taxonomy into another object 

noncontam_comm <- t(as.data.frame(noncontam_asv))
noncontam_taxo <- as.data.frame(noncontam_taxa)[colnames(noncontam_comm),]

# PCA on Hellinger-transformed community data
noncontam.pca <- prcomp(decostand(noncontam_comm,"hellinger"))
ordiplot(noncontam.pca, display="sites", type="text",cex=1.0)

# Our samples are pretty scattered with some grouped together
# Let's save the plot

pdf("/home/dpatel/sequences/Bac/bac_Figures/PCA.pdf")
ordiplot(noncontam.pca, display="sites", type="text",cex=1.0)
dev.off()

```



## Making rarefaction curves 

```{r}

# What is the smallest number of sequences/sample for subset of samples?

min(rowSums(noncontam_comm))
rowSums(noncontam_comm)

# Rarefaction curve for subset of samples

rarecurve(noncontam_comm, step=200, label=TRUE)
rarecurve(noncontam_comm, step=200, label=TRUE, xlim=c(0,3000))

# Save the graphs
rarecurve(noncontam_comm, step=200, label=TRUE)
dev.print(width = 24, height = 10, pdf,"/home/dpatel/sequences/Bac/bac_Figures/rarefaction_curve_all.pdf")

rarecurve(noncontam_comm, step=200, label=TRUE, xlim=c(0,2000))
dev.print(width = 24, height = 10, pdf,"/home/dpatel/sequences/Bac/bac_Figures/rarefaction_curve_short.pdf")

minTotRelAbun = 5e-5
x = taxa_sums(final.ps)
keepTaxa =  (x / sum(x)) > minTotRelAbun
prunedSet = prune_taxa(keepTaxa, final.ps)
sample_sums(prunedSet)
sample_sums(final.ps)

rarecurve(t(otu_table(final.ps)), step=50, cex=0.5)
ps_raref <-rarefy_even_depth(prunedSet, 2300, rngseed=112) 

#samples removed because they contained fewer reads than `sample.size`: R-J1B, R-J1M, R-J2H, R-J8H, R-R1H

#Save asv, taxa, metadata rarefied
write.csv(as.data.frame(as(tax_table(ps_raref), "matrix")), file = "~/sequences/Bac/bac_csv/rarefied_taxa.csv") 
write.csv(as.data.frame(as(otu_table(ps_raref), "matrix")),file = "~/sequences/Bac/bac_csv/rarefied_asv.csv")
write.csv(as.data.frame(as(sample_data(ps_raref), "matrix")), file="~/sequences/Bac/bac_csv/rarefied_meta.csv") 

```



